{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "What's covered: \n",
    "* Fundamentals of Q-Learning Algorithm\n",
    "* A theoretical example of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Fundamentals](#1)\n",
    "    - [1.1 - Bellmann Optimality Equation](#1.1)\n",
    "    - [1.2 - Q-Learning Objective](#1.2)\n",
    "    - [1.3 - Q-Learning with Value Iteration](#1.3)\n",
    "    - [1.4 - Storing Q-Values in a Q-Table](#1.4)\n",
    "    - [1.5 - Taking actions - Exploration Vs. Exploitation](#1.5)\n",
    "        - [1.5.1 - Epsilon Greedy Strategy](#1.5.1)\n",
    "    - [1.6 - Updating the Q-value](#1.6)\n",
    "    - [1.7 - The Learning Rate](#1.7)\n",
    "    - [1.8 - Calculating the new Q-value](#1.8)\n",
    "- [2.0 - Theoretical Example - The Lizard Game](#2.0)\n",
    "    - [2.1 - The Set Up](#2.1)\n",
    "    - [2.2 - Q-table](#2.2)\n",
    "    - [2.3 - Episodes](#2.3)\n",
    "    - [2.4 - Calculating the new Q-value](#2.4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99040702",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 - Bellmann Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122216aa",
   "metadata": {},
   "source": [
    "One fundamental property of $q_{*}$ is that it must satisfy the following equation:\n",
    "\n",
    "$$q_{*}(s,a)=E[R_{t+1} + \\gamma \\max_{a'} q_{*}(s', a')]$$\n",
    "\n",
    "This is called the *Bellmann optimality equation*. It says, that for any state-action-pair $(s, a)$ at time $t$, the expected return from starting in state $s$, selecting action $a$ and following the optimal policy thereafter (the Q-value of this pair) is going to be the expected reward we get from taking action $a$ in state $s$, which is $R_{t+1}$, plus the maximum expected discounted return that can be achieved from any possible next state-action-pair $(s', a')$\n",
    "\n",
    "Since the agent is following an optimal policy, the following state $s'$ will be the state from which the best possible next action $a'$ can be taken at time $t+1$.\n",
    "\n",
    "In the following sections we're going to see how we can use the Bellmann equation to find $q_{*}$. Once we have $q_{*}$, we can determine the optimal policy because, with $q_{*}$, for any state $s$, a reinforcement learning algorithm can finde the action $a$ that maximizes $q_{*}(s, a)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345cf64",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## 1.2 - Q-Learning Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb331a1c",
   "metadata": {},
   "source": [
    "Q-learning is a technique that can solve for the optimal policy in an MDP. The objective of Q-learning is to find a policy that is optimal in the sense that the expected value of the total reward over all successive steps is the maximum achievable. So, in other word, the goal of Q-learning is to find the optimal policy by learning the optimal Q-values for each state-action-pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ace50",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## 1.3 - Q-Learning with Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d14a40",
   "metadata": {},
   "source": [
    "The Q-function for a given policy accepts a state and an action and returns the expected return from taking the given action in the given state and following the given policy thereafter.\n",
    "\n",
    "The Q-learning algorithm iteratively updates the Q-values for each state-action pair using the Bellman equation until the Q-function converges to the optimal Q-function, $q_{*}$. This approach is called value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938eb88b",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## 1.4 - Storing Q-Values in a Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee72477",
   "metadata": {},
   "source": [
    "To store the Q-values for each state-action pair, a table, called the Q-table can be used. The horizontal axis of the table represents the actions, and the vertical axis represents the states. So, the dimension of the table are the number of actions by the number of states.\n",
    "    \n",
    "||Action 1|Action 2|Action 3|Action 4|\n",
    "|----|----|----|----|----|\n",
    "|State 1|0|0|0|0|\n",
    "|State 2|0|0|0|0|\n",
    "|State 4|0|0|0|0|\n",
    "|State 5|0|0|0|0|\n",
    "|State 6|0|0|0|0|\n",
    "    \n",
    "First all Q-values in the table are initialized to zero. Over time, though, as agent plays severals episodes, the Q-values produced for the state-action pairs that agent experiences will be used to update the Q-values stored in the Q-table.\n",
    "    \n",
    "As the Q-table becomes updated, in later moves and later episodes, the agent can look in the Q-table and base its next action on the highest Q-value for the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd57385",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "## 1.5 - Taking actions - Exploration vs. Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57eab5",
   "metadata": {},
   "source": [
    "To interact with the environment the agent has to select actions. There are two ways to take an action, which are explained below.\n",
    "\n",
    "Exploitation is the act of exploiting the information that is already known about the environment (Q-table). The agent then selects the action based on the maximum value of those actions.\n",
    "\n",
    "Exploration is the act of exploring the environment to find out information about it. Instead of selecting actions based on the maximum future reward we select an action randomly. This allows the agent to discover new states that otherwise may not be seen during the exploitation process.\n",
    "\n",
    "The goal of an agent is to maximize the expected return, so you might think we want our agent to use exploitation all the time and not worry about doing any exploration. This strategy, however, isn't quite right.\n",
    "\n",
    "Let's explain this with an example.\n",
    "\n",
    "You moved to a new city and want to find your favourite restaurant. In the first week you tried five different restaurants out of 100 in total. In the second week you want to go to dinner again. When choosing a restaurant now, you can decide whether you go to your, based on your experience you gained until now, favourite restaurant (exploitation) or try a new one (exploration). If you choose your favourite restaurant, you might miss a restaurant that you like even better than your current favourite restaurant. On the other hand, if you decide to choose a new restaurant, there is a risk that the newly tried restaurant is not good.\n",
    "\n",
    "Humans try to get as much information as possible before making a move, for example before we try a new restaurant we read reviews or ask friends who already tried it. In Reinforcement Learning on the other hand, it is not possible to do that, but there are some techniques that will help figuring out the best stretegy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2444cd4",
   "metadata": {},
   "source": [
    "<a name='1.5.1'></a>\n",
    "### 1.5.1 - Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50613fa",
   "metadata": {},
   "source": [
    "To get a balance between exploitation and exploration, *epsilon greedy strategy* can be used. With this strategy, we define an exploration rate $\\epsilon$ that we initially set to 1. This exploration rate is the probability that our agent will explore the environment rather than exploit it. With $\\epsilon = 1$, it is 100% certain that the agent will start out by exploring the environment.\n",
    "    \n",
    "As the agent learns more about the environment, at the start of each new episode, $\\epsilon$ will decay by some rate that we set, so that the likelihood of exploration becomes less and less probable as the agent learns more and more about the environment. The agent will become \"greedy\" in terms of exploiting the environment once it has had the opportunity to explore and learn more about it.\n",
    "    \n",
    "    \n",
    "<img src=\"images/exploration_vs_exploitation.png\" style=\"width:400;height:400px;\">\n",
    "<caption><center><font><b>Figure 2</b>: Exploration Vs. Exploitation trade-off</center></caption>\n",
    "    \n",
    "To determine whether the agent will choose exploration at each time step, we generate a random number between 0 and 1. If this number is greater than epsilon, then the agent will choose its next action via exploitation, i.e. it will choose the action with the highest Q-value for its current state from the Q-table. Otherweise, its next action will be chosen via exploration, i.e. randomly choosing its action and exploring what happens in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5a6f6",
   "metadata": {},
   "source": [
    "<a name='1.6'></a>\n",
    "### 1.6 - Updating the Q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf258b",
   "metadata": {},
   "source": [
    "To update the Q-value for the action taken from the previous state, we use the Bellman equation that we highlighted previously:\n",
    "    \n",
    "$$q_{*}(s,a)=E[R_{t+1} + \\gamma \\max_{a'} q_{*}(s', a')]$$\n",
    "\n",
    "We want to make the Q-value for the given state-action pair as close as we can to the right hand side of the Bellman equation so that the Q-value will eventually converge to the optimal Q-value $q_{*}$.\n",
    "    \n",
    "This will happen over time by iteratively comparing the loss between the Q-value and the optimal Q-value for the given state-action pair and then updating the Q-value over and over again each time we encounter this same state-action pair to reduce the loss.\n",
    "    \n",
    "$$q_{*}(s,a) - q(s,a)=loss$$\n",
    "    \n",
    "$$E[R_{t+1} + \\gamma \\max_{a'} q_{*}(s', a')] - E[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}]$$\n",
    "    \n",
    "So actually see how we update the Q-value, we first need to introduce the idea of a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245df127",
   "metadata": {},
   "source": [
    "<a name='1.7'></a>\n",
    "### 1.7 - The Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7beb12",
   "metadata": {},
   "source": [
    "The learning rate is a number between 0 and 1, which can be thought of as how quickly the agent abandons the previous Q-value in the Q-table for a given state-action pair for the new Q-value.\n",
    "    \n",
    "Suppose we have a Q-value in the Q-table for some arbitrary state-action pair that the agent has experienced in a previous time step. Well, if the agent experiences that same state-action pair at a later time step once it's learned more about the environment, the Q-value will need to be updated to reflect the change in expectations the agent now has for the future returns.\n",
    "    \n",
    "We don't want to just overwrite the old Q-value, but rather, we use the learning rate as a tool to determine how much information we keep about the previously computed Q-value for the given state-action pair versus the new Q-value calculated for the same state-action pair at a later time step. We'll denote the learning rate with the symbol $\\alpha$.\n",
    "    \n",
    "The higher the learning rate, the more quickly the agent will adopt the new Q-value. For example, if the learning rate is 1, the estimate for the Q-value for a given state-action pair would be the straight up newly calculated Q-value and would not consider previous Q-values that had been calculated for the given state-action pair at previous time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673dcaa",
   "metadata": {},
   "source": [
    "<a name='1.8'></a>\n",
    "### 1.8 - Calculating the new Q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5aa999",
   "metadata": {},
   "source": [
    "The formula for calculating the new Q-value for the state-action pair (s,a) at time t is this:\n",
    "    \n",
    "$$q^{new}(s,a)=(1-\\alpha) q(s,a) + \\alpha(R_{t+1} + \\gamma \\max_{a'} q(s', a'))$$\n",
    "\n",
    "So, our new Q-value is equal to a weighted sum of our old value and the learned value.\n",
    "    \n",
    "Our learned value is the reward the agent receives from the taken action plus the discounted estimate of the optimal future Q-value for the next state-action pair (s',a') at time t+1. This entire learned value is then multiplied by our learning rate.\n",
    "    \n",
    "Alright, so now we'll take this new Q-value we just calculated and store it in our Q-table for this particular state-action pair.\n",
    "    \n",
    "This same process will happen for each time step until termination in each episode.\n",
    "    \n",
    "Once the Q-function converges to the optimal Q-function, we will have our optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731aa80e",
   "metadata": {},
   "source": [
    "<a name='2.0'></a>\n",
    "# 2.0 - Theoretical Example - The Lizard Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229dcf21",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "## 2.1 - The Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880bdbc",
   "metadata": {},
   "source": [
    "Suppose we have following environment shown below. The agent in our environment is the lizard. The lizard wants to eat as many crickets as possible in the least amount of time without stumbling across a bird, which will, itself, eat the lizard.\n",
    "\n",
    "<img src=\"images/lizard.png\" style=\"width:400;height:400px;\">\n",
    "<caption><center><font ><b>Figure 1</b>: The Lizard Game </center></caption>\n",
    "    \n",
    "The lizard can move left, right, up or down in this environment. These are the actions. The states are determined by the individual tiles and where the lizard is on the board at any given time.\n",
    "    \n",
    "If the lizard lands on a tile has one cricket, the reward is plus one point. Landing on an empty tile is minus one point. A tile with five crickets is plus ten points and will end the episode. A tile with a bird is minus ten points and will also end the episode. \n",
    "    \n",
    "|State|Reward|\n",
    "|----|----|\n",
    "|One cricket|+1|\n",
    "|Empty|-1|\n",
    "|Five crickets|+10 Game over|\n",
    "|Bird|-10 Game over|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a0d56",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "## 2.2 - Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c2c7c",
   "metadata": {},
   "source": [
    "Now, at the start of the game, the lizard has no idea how good any given action is from any given state. It's not aware of anything besides the current state of the environment. In other word, it doesn't know from the start whether navigating left, right, up, or down will result in a positive reward or negative reward.\n",
    "    \n",
    "Therefore, the Q-values for each state-action pair will all be initialized to zero since the lizard knows nothing about the environment at the start. Throughout the game, though, the Q-values will be iteratively updated using value iteration.\n",
    "    \n",
    "||Left|Right|Up|Down|\n",
    "|----|----|----|----|----|\n",
    "|1 cricket|0|0|0|0|\n",
    "|Empty 1|0|0|0|0|\n",
    "|Empty 2|0|0|0|0|\n",
    "|Empty 3|0|0|0|0|\n",
    "|Bird|0|0|0|0|\n",
    "|Empty 4|0|0|0|0|\n",
    "|Empty 5|0|0|0|0|\n",
    "|Empty 6|0|0|0|0|\n",
    "|5 crickets|0|0|0|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85c33b",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "## 2.3 - Episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7c104",
   "metadata": {},
   "source": [
    "Now, we'll set some standard number of episodes that we want the lizard to play. Let's say we want the lizard to play five episodes. It is during these episodes that the learning process will take place.\n",
    "    \n",
    "In each episode, the lizard starts out by choosing an action from the starting state based on the current Q-values in the table. Well at the beginning, we should know that the action will be choosen randomly via exploration since our exploration rate is set to 1 inititaly. Meaning, with 100% probability, the lizard will explore the environment during the frist episode of the game, rather than exploit it.\n",
    "\n",
    "Alright, so after the lizard takes an action, it observes the next state, the reward gained from its action, and updates the Q-value in the Q-table for the action it took from previous state.\n",
    "\n",
    "Let's suppose the lizard chooses to move right as its action from the starting state. We can see the reward we get in this new state is -1 since, recall, empty tiles have a reward of -1 point.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a78ca",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 - Calculating the new Q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5374a",
   "metadata": {},
   "source": [
    "To calculate the new Q-Value for the state-action pair (s,a) we first need to choose a learning rate and a discount rate. For this example we choose the learning rate to be $\\alpha = 0.7$ and the discount rate to be $\\gamma = 0.99$. The formula for calculating the new Q-value for the state-action pair (s,a) at time t is this:\n",
    "    \n",
    "$$q^{new}(s,a)=(1-\\alpha) q(s,a) + \\alpha(R_{t+1} + \\gamma \\max_{a'} q(s', a'))$$\n",
    "\n",
    "So, our new Q-value is equal to a weighted sum of our old value and the learned value. The old value in our case is 0 since this frist time the agent is experiencing this particular state-action pair, and we multiply this old value by $(1-\\alpha)$.\n",
    "    \n",
    "Our learned value is the reward the agent receives from moving right from the starting state plus the discounted estimate of the optimal future Q-value for the next state-action pair (s',a') at time t+1. This entire learned value is then multiplied by our learning rate.\n",
    "        \n",
    "$$q^{new}(s,a)=(1-\\alpha) q(s,a) + \\alpha(R_{t+1} + \\gamma \\max_{a'} q(s', a'))$$\n",
    "\n",
    "$$=(1-0.7) (0) + 0.7(-1 + 0.99 (\\max_{a'} q(s', a'))$$\n",
    "    \n",
    "Let's pause for a moment and focus on the term $\\max_{a'} q(s', a')$. Since all the Q-values are currently initialized to 0 in the Q-table, we have\n",
    "    \n",
    "$$\\max_{a'} q(s', a')= \\max(q(empty6, left), q(empty6, right), q(empty6, up), q(empty6, down))$$\n",
    "    \n",
    "$$=\\max(0,0,0,0)$$\n",
    "\n",
    "$$=0$$\n",
    "\n",
    "Now, we can substitute the value 0 in for $\\max_{a'} q(s', a')$ in our earlier equation to solve for $q^{new}(s,a)$.\n",
    "    \n",
    "$$q^{new}(s,a)=(1-\\alpha) q(s,a) + \\alpha(R_{t+1} + \\gamma \\max_{a'} q(s', a'))$$\n",
    "\n",
    "$$=(1-0.7) (0) + 0.7(-1 + 0.99 (\\max_{a'} q(s', a'))$$\n",
    "    \n",
    "$$=(1-0.7) (0) + 0.7(-1 + 0.99 (0))$$\n",
    "    \n",
    "$$=0 + 0.7(-1)$$\n",
    "    \n",
    "$$=-0.7$$\n",
    "    \n",
    "Alright, so now we'll take this new Q-value we just calculated and store it in our Q-table for this particular state-action pair.\n",
    "\n",
    "||Left|Right|Up|Down|\n",
    "|----|----|----|----|----|\n",
    "|1 cricket|0|0|0|0|\n",
    "|Empty 1|0|-0.7|0|0|\n",
    "|Empty 2|0|0|0|0|\n",
    "|Empty 3|0|0|0|0|\n",
    "|Bird|0|0|0|0|\n",
    "|Empty 4|0|0|0|0|\n",
    "|Empty 5|0|0|0|0|\n",
    "|Empty 6|0|0|0|0|\n",
    "|5 crickets|0|0|0|0|\n",
    "    \n",
    "We've now done everything needed for a single time step. This same process will happen for each time step until termination in each episode.\n",
    "\n",
    "Oh, and speaking of termination, we can also specify a max number of steps that our agent can take before the episode auto-terminates. With the way the game is set up right now, termination will only occur if the lizard reaches the state with five crickets or the state with the bird.\n",
    "    \n",
    "Once the Q-function converges to the optimal Q-function, we will have our optimal policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
